---
title: "Linear Regression"
author: "Brayan Duran Medina"
output: html_document
---

# 0) Importing libraries
```{r}
library(readxl)
library(dplyr)
library(ggplot2)
library(GGally)
library(ggcorrplot)
library(MASS)
library(lmtest)
```

# 1) Plot the data, analyze if predictors need to be transform, check for outliers

## Read description and data from Excel file
```{r}
description <- read_excel("diamonds.xlsx", sheet = "description")
data <- read_excel("diamonds.xlsx", sheet = "data")

print(description)
print(data)
```

## Add diamond classification column
```{r}
data <- data %>%
  mutate(diamond_classification = case_when(
    carat < 0.3 ~ "small",
    carat >= 0.3 & carat < 1 ~ "medium",
    carat >= 1 ~ "large"
  ))
```

## Columns names
```{r}
column_names <- names(data)

print(column_names)
```

## Tranform data from string to numeric
```{r}

data <- data %>%
  mutate(cut = case_when(
    cut == "Fair" ~ 1,
    cut == "Good" ~ 2,
    cut == "Very Good" ~ 3,
    cut == "Premium" ~ 4,
    cut == "Ideal" ~ 5,
  ))

data <- data %>%
  mutate(color = case_when(
    color == "J" ~ 1,
    color == "I" ~ 2,
    color == "H" ~ 3,
    color == "G" ~ 4,
    color == "F" ~ 5,
    color == "E" ~ 6,
    color == "D" ~ 7,
  ))

data <- data %>%
  mutate(clarity = case_when(
    clarity == "I1" ~ 1,
    clarity == "SI1" ~ 2,
    clarity == "SI2" ~ 3,
    clarity == "VS1" ~ 4,
    clarity == "VS2" ~ 5,
    clarity == "VVS1" ~ 6,
    clarity == "VVS2" ~ 7,
    clarity == "IF" ~ 8,
  ))

data <- data %>%
  mutate(diamond_classification = case_when(
    diamond_classification == "small" ~ 1,
    diamond_classification == "medium" ~ 2,
    diamond_classification == "large" ~ 3
  ))

for (var in column_names) {
  data[[var]] <- as.numeric(data[[var]])
}

data
```

## Make Pairplot
```{r}
ggpairs(data, columns = c("carat", "cut", "color", "clarity",
                          "depth", "table", "price", "x", "y", "z"))
```

## Make Correlation Plot
```{r}
cor_mat <- cor(data)
ggcorrplot(cor_mat)
```

## Check for outliers using boxplot
```{r}
for (var in column_names) {
  boxplot(data[[var]], main = var)
}
```

# 2) Create the first model
```{r}
data <- data.frame(lapply(data, as.numeric))

lm_model1 <- lm(price ~ . - diamond_classification, data = data)
```

# 3) Plot the residuals of the first model
```{r}
residuals_data <- residuals(lm_model1)

plot(residuals_data, main = "Residuals of the model 1")
abline(h = 0, col = "red")
```

# 4) Kolmogorov-Smirnov test for first model
```{r}
residuals_data_standardized <- scale(residuals_data)
ks_test_residuals <- ks.test(residuals_data_standardized, "pnorm")

print(ks_test_residuals)

data <- data.frame(lapply(data, as.numeric))

ks_results <- list()

for (var in names(data)) {
  p <- ggplot(data, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "blue", color = "black") +
    ggtitle(paste("Histogram of", var))
  print(p)

  data_standardized <- scale(data[[var]])

  ks_results[[var]] <- ks.test(data_standardized, "pnorm")
}

ks_results
```

# 5) Box-Cox Transformation for first model

## Apply Box-Cox transformation to variables with p-value < 0.05
```{r}

data_with_boxcox <- data

for (var in names(data)) {
  if (ks_results[[var]]$p.value < 0.05) {
    if (all(data[[var]] > 0)) {
      fmla <- as.formula(paste(var, "~ ."))

      bc_result <- boxcox(lm(fmla, data = data))
      lambda_optimal <- bc_result$x[which.max(bc_result$y)]

      data_with_boxcox[[var]] <-
        (data[[var]]^lambda_optimal - 1) / lambda_optimal
    } else {
      message(paste("The variable",
                    var,
                    "contains non-positivevalues and Box-Cox was not applied."))
    }
  }
}
```

## Update the first model with the data transformed
```{r}
lm_model1 <- lm(price ~ . - diamond_classification, data = data_with_boxcox)
```

# 6) Plot the residuals after Box-Cox transformation for first model
```{r}
residuals_data <- residuals(lm_model1)

plot(residuals_data, main = "Residuals of the model")
abline(h = 0, col = "red")
```

# 7) Breusch-Pagan test for heteroscedasticity for first model
```{r}
bptest_result <- bptest(lm_model1)
print(bptest_result)
```

# 8) Calculate Cook's distance for first model
```{r}
cooks_distances <- cooks.distance(lm_model1)

plot(cooks_distances, type = "h", main = "Cook's distances",
     ylab = "Cook's distance", xlab = "Observation Index")

abline(h = 4 / (nrow(data) - length(coef(lm_model1))), col = "red")
```

# 9) Create second lineal model
```{r}
data <- data.frame(lapply(data, as.numeric))

lm_model2 <- lm(price ~ . - carat, data = data)
```

# 10) Plot the residuals of the second model
```{r}
residuals_data <- residuals(lm_model2)

plot(residuals_data, main = "Residuals of the model 2")
abline(h = 0, col = "red")
```

# 11) Kolmogorov-Smirnov test for second model
```{r}
residuals_data_standardized <- scale(residuals_data)
ks_test_residuals <- ks.test(residuals_data_standardized, "pnorm")

print(ks_test_residuals)

data <- data.frame(lapply(data, as.numeric))

ks_results <- list()

for (var in names(data)) {
  p <- ggplot(data, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "blue", color = "black") +
    ggtitle(paste("Histogram of", var))
  print(p)

  data_standardized <- scale(data[[var]])

  ks_results[[var]] <- ks.test(data_standardized, "pnorm")
}

ks_results
```

# 12) Box-Cox Transformation for second model

## Apply Box-Cox transformation to variables with p-value < 0.05
```{r}

data_with_boxcox <- data

for (var in names(data)) {
  if (ks_results[[var]]$p.value < 0.05) {
    if (all(data[[var]] > 0)) {
      fmla <- as.formula(paste(var, "~ ."))

      bc_result <- boxcox(lm(fmla, data = data))
      lambda_optimal <- bc_result$x[which.max(bc_result$y)]

      data_with_boxcox[[var]] <-
        (data[[var]]^lambda_optimal - 1) / lambda_optimal
    } else {
      message(paste("The variable",
                    var,
                    "contains non-positivevalues and Box-Cox was not applied."))
    }
  }
}
```

## Update the second model with the data transformed
```{r}
lm_model2 <- lm(price ~ . - carat, data = data_with_boxcox)
```

# 13) Plot the residuals after Box-Cox transformation for second model
```{r}
residuals_data <- residuals(lm_model2)

plot(residuals_data, main = "Residuals of the model")
abline(h = 0, col = "red")
```

# 14) Breusch-Pagan test for heteroscedasticity for second model
```{r}
bptest_result <- bptest(lm_model2)
print(bptest_result)
```

# 15) Calculate Cook's distance for second model
```{r}
cooks_distances <- cooks.distance(lm_model2)

plot(cooks_distances, type = "h", main = "Cook's distances",
     ylab = "Cook's distance", xlab = "Observation Index")

abline(h = 4 / (nrow(data) - length(coef(lm_model2))), col = "red")
```

# 16) Davidson-MacKinnon J test to compare models
```{r}
jtest_result <- jtest(lm_model1, lm_model2)

print(jtest_result)
```

# 17) Conclusion

1. **Data Transformation and Analysis**:
   - The data was successfully transformed from categorical to numeric, particularly for the variables 'cut', 'color', 'clarity', and a newly added 'diamond_classification'.
   - Pairplots and correlation plots were generated, offering a visual understanding of the relationships between variables. Notably, boxplots were used to identify potential outliers.

2. **Model Development and Evaluation**:
   - Two linear regression models were constructed. The first model (`lm_model1`) excluded the 'diamond_classification' variable, while the second model (`lm_model2`) excluded 'carat'.
   - Residual analysis for both models revealed the distribution of errors. The Kolmogorov-Smirnov test indicated non-normal distribution of residuals for both models, as evidenced by significant p-values in all cases.

3. **Box-Cox Transformation**:
   - The Box-Cox transformation was applied to variables with significant p-values in the Kolmogorov-Smirnov test. This step aimed to improve the normality of the data.
   - However, it was noted that the variables 'x', 'y', and 'z' were not transformed due to non-positive values.

4. **Heteroscedasticity Check**:
   - The Breusch-Pagan test indicated the presence of heteroscedasticity in both models, with significant p-values suggesting non-constant variance of residuals.

5. **Influence Analysis**:
   - Cook's distance calculations identified influential observations within both models.

6. **Model Comparison Using Davidson-MacKinnon J Test**:
   - The Davidson-MacKinnon J test was used to compare the two models. The results showed that both models significantly contribute to explaining the 'price' variable. However, the addition of fitted values from `lm_model2` to `lm_model1` and vice versa resulted in significant improvements in both cases.
   - This suggests that each model has unique features that are important for predicting the 'price'. The choice of a superior model depends on specific analysis goals and the balance between model simplicity and predictive accuracy.

7. **Final Thoughts**:
   - Both models demonstrate significant predictive capabilities, but they also have distinct characteristics. The inclusion or exclusion of specific variables ('diamond_classification' and 'carat') impacts the model performance differently.
   - The presence of heteroscedasticity and the influence of certain data points (as indicated by Cook's distance) should be considered in further model refinements and applications.
   - Overall, the analysis illustrates the complexity of modeling in real-world scenarios and the importance of thorough evaluation and comparison of different models.