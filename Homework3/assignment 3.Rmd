---
title: "Linear Regression"
author: "Brayan Duran Medina"
output: html_document
---

# 0) Importing libraries
```{r}
library(readxl)
library(dplyr)
library(ggplot2)
library(GGally)
library(ggcorrplot)
library(MASS)
library(lmtest)
```

# 1) Plot the data, analyze if predictors need to be transform, check for outliers

## Read description and data from Excel file
```{r}
description <- read_excel("diamonds.xlsx", sheet = "description")
data <- read_excel("diamonds.xlsx", sheet = "data")

print(description)
print(data)
```

## Add diamond classification column
```{r}
data <- data %>%
  mutate(diamond_classification = case_when(
    carat < 0.3 ~ "small",
    carat >= 0.3 & carat < 1 ~ "medium",
    carat >= 1 ~ "large"
  ))
```

## Columns names
```{r}
column_names <- names(data)

print(column_names)
```

## Tranform Categorical variables using dummy variables
```{r}

data <- data %>%
  mutate(cut = factor(cut, levels = c("Fair",
                                      "Good",
                                      "Very Good",
                                      "Premium",
                                      "Ideal"))) %>%

  mutate(color = factor(color, levels = c("J",
                                          "I",
                                          "H",
                                          "G",
                                          "F",
                                          "E",
                                          "D"))) %>%

  mutate(clarity = factor(clarity, levels = c("I1",
                                              "SI1",
                                              "SI2",
                                              "VS1",
                                              "VS2",
                                              "VVS1",
                                              "VVS2",
                                              "IF"))) %>%

  mutate(diamond_classification =
           factor(diamond_classification, levels = c("small",
                                                     "medium",
                                                     "large")))

cut_dummies <- model.matrix(~ cut - 1, data)
color_dummies <- model.matrix(~ color - 1, data)
clarity_dummies <- model.matrix(~ clarity - 1, data)
diamond_classification_dummies <-
  model.matrix(~ diamond_classification - 1, data)

cut_dummies <- cut_dummies[, -1]
color_dummies <- color_dummies[, -1]
clarity_dummies <- clarity_dummies[, -1]
diamond_classification_dummies <- diamond_classification_dummies[, -1]

data <- cbind(data, cut_dummies,
              color_dummies,
              clarity_dummies,
              diamond_classification_dummies)

data <- data[, !(names(data) %in% c("cut",
                                    "color",
                                    "clarity",
                                    "diamond_classification"))]

column_names <- names(data)

print(names(data))
```

## Make Pairplot
```{r}
ggpairs(data, columns = c("price", "carat", "depth", "table", "x", "y", "z"))
```

## Make Correlation Plot
```{r}
cor_mat <- cor(data)
ggcorrplot(cor_mat)
```

## Check for outliers using boxplot
```{r}
for (var in column_names) {
  boxplot(data[[var]], main = var)
}
```

# 2) Create the first model
```{r}
data <- data.frame(lapply(data, as.numeric))

lm_model1 <- lm(price ~ . - diamond_classificationmedium
                - diamond_classificationlarge, data = data)
```

# 3) Plot the residuals of the first model
```{r}
residuals_data <- residuals(lm_model1)

qqnorm(residuals(lm_model1))
qqline(residuals(lm_model1), col = "red")
```

# 4) Box-Cox Transformation for first model

```{r}
bc_result <- boxcox(lm_model1)
lambda_optimal <- bc_result$x[which.max(bc_result$y)]

data$price_transformed_lm1 <- (data$price^lambda_optimal - 1) / lambda_optimal
```

# 5) Update the first model with the data transformed
```{r}
data <- data.frame(lapply(data, as.numeric))

lm_model1 <- lm(price_transformed_lm1 ~ . - diamond_classificationmedium
                - diamond_classificationlarge - price,
                data = data)
```

# 6) Plot the residuals after Box-Cox transformation for first model
```{r}
residuals_data <- residuals(lm_model1)

qqnorm(residuals(lm_model1))
qqline(residuals(lm_model1), col = "red")
```

# 7) Breusch-Pagan test for heteroscedasticity for first model
```{r}
bptest_result <- bptest(lm_model1)
print(bptest_result)
```

# 8) Calculate Cook's distance for first model
```{r, fig.width=10}
cooks_distances <- cooks.distance(lm_model1)

par(mfrow = c(1, 2))

plot(fitted(lm_model1), cooks_distances,
     xlab = "Fitted Values", ylab = "Cook's Distance",
     main = "Cook's Distance vs Fitted Values", col = "blue", pch = 19)
abline(h = 4 / (nrow(data) - length(coef(lm_model1))), col = "red")

plot(data$price_transformed_lm1, cooks_distances,
     xlab = "Transformed Values", ylab = "Cook's Distance",
     main = "Cook's Distance vs Transformed Values", col = "blue", pch = 19)
abline(h = 4 / (nrow(data) - length(coef(lm_model1))), col = "red")
```

# 9) Create second lineal model
```{r}
data <- data.frame(lapply(data, as.numeric))

lm_model2 <- lm(price ~ . - carat - price_transformed_lm1, data = data)
```

# 10) Plot the residuals of the second model
```{r}
residuals_data <- residuals(lm_model2)

qqnorm(residuals(lm_model2))
qqline(residuals(lm_model2), col = "red")
```

# 11) Box-Cox Transformation for second model

```{r}
bc_result <- boxcox(lm_model2)
lambda_optimal <- bc_result$x[which.max(bc_result$y)]

data$price_transformed_lm2 <- (data$price^lambda_optimal - 1) / lambda_optimal
```

# 12) Update the second model with the data transformed
```{r}
data <- data.frame(lapply(data, as.numeric))

lm_model2 <- lm(price_transformed_lm2 ~ . - carat - price
                - price_transformed_lm1,
                data = data)
```

# 13) Plot the residuals after Box-Cox transformation for second model
```{r}
residuals_data <- residuals(lm_model2)

qqnorm(residuals(lm_model2))
qqline(residuals(lm_model2), col = "red")
```

# 14) Breusch-Pagan test for heteroscedasticity for second model
```{r}
bptest_result <- bptest(lm_model2)
print(bptest_result)
```

```{r, fig.width=10}
cooks_distances <- cooks.distance(lm_model2)

par(mfrow = c(1, 2))

plot(fitted(lm_model2), cooks_distances,
     xlab = "Fitted Values", ylab = "Cook's Distance",
     main = "Cook's Distance vs Fitted Values", col = "blue", pch = 19)
abline(h = 4 / (nrow(data) - length(coef(lm_model2))), col = "red")

plot(data$price_transformed_lm2, cooks_distances,
     xlab = "Transformed Values", ylab = "Cook's Distance",
     main = "Cook's Distance vs Transformed Values", col = "blue", pch = 19)
abline(h = 4 / (nrow(data) - length(coef(lm_model2))), col = "red")
```

# 16) Davidson-MacKinnon J test to compare models
```{r}
jtest_result <- jtest(lm_model1, lm_model2)

print(jtest_result)
```

# 17) Conclusion

1. **Data Transformation and Analysis**:
   - The data was successfully transformed from categorical to numerical format using dummy variables.
   - Pairplots and correlation plots were generated, offering a visual understanding of the relationships between variables. Notably, boxplots were used to identify potential outliers.

2. **Model Development and Evaluation**:
   - Two linear regression models were constructed. The first model (`lm_model1`) excluded the 'diamond_classification' categories, while the second model (`lm_model2`) excluded 'carat'.
   - Residual analysis for both models revealed the distribution of errors. The Kolmogorov-Smirnov test indicated non-normal distribution of residuals for both models, as evidenced by significant p-values in all cases.

3. **Box-Cox Transformation**:
   - The Box-Cox transformation was applied to variables with significant p-values in the Kolmogorov-Smirnov test. This step aimed to improve the normality of the data.
   - However, it was noted that the variables 'x', 'y', and 'z' were not transformed due to non-positive values.

4. **Heteroscedasticity Check**:
   - The Breusch-Pagan test indicated the presence of heteroscedasticity in both models, with significant p-values suggesting non-constant variance of residuals.

5. **Influence Analysis**:
   - Cook's distance calculations identified influential observations within both models.

6. **Model Comparison Using Davidson-MacKinnon J Test**:
   - The Davidson-MacKinnon J test was used to compare the two models. The results showed that both models significantly contribute to explaining the 'price' variable. However, the addition of fitted values from `lm_model2` to `lm_model1` and vice versa resulted in significant improvements in both cases.
   - This suggests that each model has unique features that are important for predicting the 'price'. The choice of a superior model depends on specific analysis goals and the balance between model simplicity and predictive accuracy.

7. **Final Thoughts**:
   - Both models demonstrate significant predictive capabilities, but they also have distinct characteristics. The inclusion or exclusion of specific variables ('diamond_classification' and 'carat') impacts the model performance differently.
   - The presence of heteroscedasticity and the influence of certain data points (as indicated by Cook's distance) should be considered in further model refinements and applications.
   - Overall, the analysis illustrates the complexity of modeling in real-world scenarios and the importance of thorough evaluation and comparison of different models.